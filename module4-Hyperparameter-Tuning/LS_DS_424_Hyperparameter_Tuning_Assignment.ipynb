{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_424_Hyperparameter_Tuning_Assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9Ryp-TVm4njD"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "\n",
        "# Hyperparameter Tuning\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 4*\n",
        "\n",
        "## Your Mission, should you choose to accept it...\n",
        "\n",
        "To hyperparameter tune and extract every ounce of accuracy out of this telecom customer churn dataset: [Available Here](https://lambdaschool-data-science.s3.amazonaws.com/telco-churn/WA_Fn-UseC_-Telco-Customer-Churn+(1).csv)\n",
        "\n",
        "## Requirements\n",
        "\n",
        "- Load the data\n",
        "- Clean the data if necessary (it will be)\n",
        "- Create and fit a baseline Keras MLP model to the data.\n",
        "- Hyperparameter tune (at least) the following parameters:\n",
        " - batch_size\n",
        " - training epochs\n",
        " - optimizer\n",
        " - learning rate (if applicable to optimizer)\n",
        " - momentum (if applicable to optimizer)\n",
        " - activation functions\n",
        " - network weight initialization\n",
        " - dropout regularization\n",
        " - number of neurons in the hidden layer\n",
        " \n",
        " You must use Grid Search and Cross Validation for your initial pass of the above hyperparameters\n",
        " \n",
        " Try and get the maximum accuracy possible out of this data! You'll save big telecoms millions! Doesn't that sound great?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLS9MnW3qx7d",
        "colab_type": "code",
        "outputId": "3a8f7b65-b324-4374-cc5c-0be2e6ad444e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NNJ-tOBs4jM1",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('https://lambdaschool-data-science.s3.amazonaws.com/telco-churn/WA_Fn-UseC_-Telco-Customer-Churn+(1).csv')\n",
        "df.drop('customerID', axis=1, inplace=True)\n",
        "# df.drop('gender', axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x30Rxn9ofUXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[['Partner','Dependents','PhoneService','MultipleLines','Churn']] = df[['Partner','Dependents','PhoneService','MultipleLines','Churn']].replace(to_replace=['No', 'Yes', 'No phone service'], value=[0, 1, 2])\n",
        "df['gender'] = df['gender'].replace(to_replace=['Male', 'Female'], value=[0, 1])\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'],errors='coerce')\n",
        "df = df.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HsnNZ2GdmlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# train, val = train_test_split(df, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtKr_x7FgwjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = df.loc[:, df.columns != 'Churn']\n",
        "#X_val = val.loc[:, df.columns != 'Churn']\n",
        "y_train = df['Churn']\n",
        "#y_val = val['Churn']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBUGEQW_lFmg",
        "colab_type": "code",
        "outputId": "b134d13b-9668-4bed-c517-3c050e236fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "!pip install category-encoders"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting category-encoders\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/52/c54191ad3782de633ea3d6ee3bb2837bda0cf3bc97644bb6375cf14150a0/category_encoders-2.1.0-py2.py3-none-any.whl (100kB)\n",
            "\r\u001b[K     |███▎                            | 10kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 61kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from category-encoders) (1.18.2)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from category-encoders) (0.5.1)\n",
            "Requirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from category-encoders) (0.10.2)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category-encoders) (0.25.3)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from category-encoders) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category-encoders) (0.22.2.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.1->category-encoders) (1.12.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category-encoders) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category-encoders) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category-encoders) (0.14.1)\n",
            "Installing collected packages: category-encoders\n",
            "Successfully installed category-encoders-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV4JG6rxhqn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import category_encoders as ce\n",
        "encoder = ce.OneHotEncoder(use_cat_names=True)\n",
        "X_train_encoded = encoder.fit_transform(X_train)\n",
        "#X_val_encoded = encoder.transform(X_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RN8EVuDjKwH",
        "colab_type": "code",
        "outputId": "a94d8d24-6db0-41b5-bb7b-7817a636d080",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "y_train"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       0\n",
              "1       0\n",
              "2       1\n",
              "3       0\n",
              "4       1\n",
              "       ..\n",
              "7038    0\n",
              "7039    0\n",
              "7040    0\n",
              "7041    1\n",
              "7042    0\n",
              "Name: Churn, Length: 7032, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mgjALzPocyX",
        "colab_type": "code",
        "outputId": "63eb5967-46ae-463a-fd42-69e8c01865dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras import optimizers\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwfhcWV8o-KR",
        "colab_type": "code",
        "outputId": "21cb96e0-edf2-48de-ad30-c0e64acf6424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "  \n",
        "def create_model():  \n",
        "  model = Sequential()\n",
        "  model.add(Dense(40, input_dim=39, activation='sigmoid'))\n",
        "  model.add(Dense(40, activation='relu'))\n",
        "  model.add(Dense(40, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # Compile model\n",
        "  model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "param_grid = {'batch_size': [40],\n",
        "              'epochs': [20]}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=5)\n",
        "grid_result = grid.fit(X_train_encoded.values, y_train.values)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.7966431856155396 using {'batch_size': 40, 'epochs': 20}\n",
            "Means: 0.7819980502128601, Stdev: 0.007059935458517922 with: {'batch_size': 10, 'epochs': 20}\n",
            "Means: 0.784982419013977, Stdev: 0.009851072232779423 with: {'batch_size': 20, 'epochs': 20}\n",
            "Means: 0.7966431856155396, Stdev: 0.007283449695814187 with: {'batch_size': 40, 'epochs': 20}\n",
            "Means: 0.7903862118721008, Stdev: 0.004350836871280571 with: {'batch_size': 60, 'epochs': 20}\n",
            "Means: 0.7930890202522278, Stdev: 0.009282803947650754 with: {'batch_size': 80, 'epochs': 20}\n",
            "Means: 0.7849830269813538, Stdev: 0.0039038732284533387 with: {'batch_size': 100, 'epochs': 20}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BzfdjEko2yL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "0c8aa549-8134-48d1-f111-db030b7ccce8"
      },
      "source": [
        "# optimizer\n",
        "def create_model(optimizer='adam'):  \n",
        "  model = Sequential()\n",
        "  model.add(Dense(40, input_dim=39, activation='sigmoid'))\n",
        "  model.add(Dense(40, activation='relu'))\n",
        "  model.add(Dense(40, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # Compile model\n",
        "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "param_grid = {'batch_size': [40],\n",
        "              'epochs': [20],\n",
        "              'optimizer': ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=5)\n",
        "grid_result = grid.fit(X_train_encoded.values, y_train.values)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.798491382598877 using {'batch_size': 40, 'epochs': 20, 'optimizer': 'Adamax'}\n",
            "Means: 0.7434563636779785, Stdev: 0.01348211700289073 with: {'batch_size': 40, 'epochs': 20, 'optimizer': 'SGD'}\n",
            "Means: 0.7828462958335877, Stdev: 0.019805662701765543 with: {'batch_size': 40, 'epochs': 20, 'optimizer': 'RMSprop'}\n",
            "Means: 0.7727537870407104, Stdev: 0.010248221380212637 with: {'batch_size': 40, 'epochs': 20, 'optimizer': 'Adagrad'}\n",
            "Means: 0.7421798825263977, Stdev: 0.014339848383757776 with: {'batch_size': 40, 'epochs': 20, 'optimizer': 'Adadelta'}\n",
            "Means: 0.7891090989112854, Stdev: 0.010176252081079282 with: {'batch_size': 40, 'epochs': 20, 'optimizer': 'Adam'}\n",
            "Means: 0.798491382598877, Stdev: 0.007182038335872271 with: {'batch_size': 40, 'epochs': 20, 'optimizer': 'Adamax'}\n",
            "Means: 0.7893895745277405, Stdev: 0.006345216902416895 with: {'batch_size': 40, 'epochs': 20, 'optimizer': 'Nadam'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha7zV5vs3U7F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "929b4f8c-00e3-45a1-bef7-140644310a10"
      },
      "source": [
        "# Activation Function\n",
        "def create_model(activation='relu', activation2='relu'):  \n",
        "  model = Sequential()\n",
        "  model.add(Dense(40, input_dim=39, activation='sigmoid'))\n",
        "  model.add(Dense(40, activation=activation))\n",
        "  model.add(Dense(40, activation=activation2))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # Compile model\n",
        "  model.compile(loss='binary_crossentropy', optimizer='Adamax', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "param_grid = {'batch_size': [40],\n",
        "              'epochs': [20],\n",
        "              #'optimizer': ['Adamax'],\n",
        "              'activation': ['relu', 'sigmoid'],\n",
        "              'activation2': ['relu', 'sigmoid']}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=5)\n",
        "grid_result = grid.fit(X_train_encoded.values, y_train.values)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.8019032001495361 using {'activation': 'sigmoid', 'activation2': 'relu', 'batch_size': 40, 'epochs': 20}\n",
            "Means: 0.7899588584899903, Stdev: 0.007214821617277054 with: {'activation': 'relu', 'activation2': 'relu', 'batch_size': 40, 'epochs': 20}\n",
            "Means: 0.8009079933166504, Stdev: 0.009272487375984507 with: {'activation': 'relu', 'activation2': 'sigmoid', 'batch_size': 40, 'epochs': 20}\n",
            "Means: 0.8019032001495361, Stdev: 0.008710333088292214 with: {'activation': 'sigmoid', 'activation2': 'relu', 'batch_size': 40, 'epochs': 20}\n",
            "Means: 0.8003390908241272, Stdev: 0.00911267864648715 with: {'activation': 'sigmoid', 'activation2': 'sigmoid', 'batch_size': 40, 'epochs': 20}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vft1ri5H5RdQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9d6efb61-115d-4676-85a0-f70839ad7c68"
      },
      "source": [
        "# learning rate\n",
        "# Activation Function\n",
        "from tensorflow.keras.optimizers import Adamax\n",
        "def create_model(learn_rate=0.01):  \n",
        "  model = Sequential()\n",
        "  model.add(Dense(40, input_dim=39, activation='sigmoid'))\n",
        "  model.add(Dense(40, activation='sigmoid'))\n",
        "  model.add(Dense(40, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # Compile model\n",
        "  model.compile(loss='binary_crossentropy', optimizer=Adamax(learning_rate=learn_rate), metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "param_grid = {'batch_size': [40],\n",
        "              'epochs': [20],\n",
        "              'learn_rate': [0.001, 0.005, .01]}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=5)\n",
        "grid_result = grid.fit(X_train_encoded.values, y_train.values)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.7955064177513123 using {'batch_size': 40, 'epochs': 20, 'learn_rate': 0.005}\n",
            "Means: 0.7943664073944092, Stdev: 0.008240732563782473 with: {'batch_size': 40, 'epochs': 20, 'learn_rate': 0.001}\n",
            "Means: 0.7955064177513123, Stdev: 0.0034324804791297067 with: {'batch_size': 40, 'epochs': 20, 'learn_rate': 0.005}\n",
            "Means: 0.7848412752151489, Stdev: 0.010957001976281832 with: {'batch_size': 40, 'epochs': 20, 'learn_rate': 0.01}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQGVoMXS9IeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, val = train_test_split(df, random_state=42)\n",
        "\n",
        "X_train = train.loc[:, train.columns != 'Churn']\n",
        "X_val = val.loc[:, val.columns != 'Churn']\n",
        "y_train = train['Churn']\n",
        "y_val = val['Churn']\n",
        "\n",
        "encoder = ce.OneHotEncoder(use_cat_names=True)\n",
        "X_train_encoded = encoder.fit_transform(X_train)\n",
        "X_val_encoded = encoder.transform(X_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO2sdw8g8Mtn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "fd1ae698-3885-466a-aafb-76072726806e"
      },
      "source": [
        "# Early Stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "stop = EarlyStopping(monitor='val_accuracy', min_delta=.00005, patience=5)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(40, input_dim=39, activation='sigmoid'))\n",
        "model.add(Dense(40, activation='sigmoid'))\n",
        "model.add(Dense(40, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adamax(learning_rate=.005), metrics=['accuracy'])\n",
        "model.fit(X_train_encoded.values, y_train.values, epochs=1000, validation_data=(X_val_encoded.values, y_val.values), callbacks=[stop])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5274 samples, validate on 1758 samples\n",
            "Epoch 1/1000\n",
            "5274/5274 [==============================] - 1s 133us/sample - loss: 0.5333 - accuracy: 0.7461 - val_loss: 0.4851 - val_accuracy: 0.7656\n",
            "Epoch 2/1000\n",
            "5274/5274 [==============================] - 0s 59us/sample - loss: 0.4762 - accuracy: 0.7710 - val_loss: 0.4720 - val_accuracy: 0.7759\n",
            "Epoch 3/1000\n",
            "5274/5274 [==============================] - 0s 57us/sample - loss: 0.4671 - accuracy: 0.7857 - val_loss: 0.4698 - val_accuracy: 0.7765\n",
            "Epoch 4/1000\n",
            "5274/5274 [==============================] - 0s 60us/sample - loss: 0.4623 - accuracy: 0.7825 - val_loss: 0.4822 - val_accuracy: 0.7736\n",
            "Epoch 5/1000\n",
            "5274/5274 [==============================] - 0s 59us/sample - loss: 0.4584 - accuracy: 0.7914 - val_loss: 0.4493 - val_accuracy: 0.7793\n",
            "Epoch 6/1000\n",
            "5274/5274 [==============================] - 0s 59us/sample - loss: 0.4480 - accuracy: 0.7907 - val_loss: 0.4508 - val_accuracy: 0.7861\n",
            "Epoch 7/1000\n",
            "5274/5274 [==============================] - 0s 58us/sample - loss: 0.4454 - accuracy: 0.7914 - val_loss: 0.4506 - val_accuracy: 0.7861\n",
            "Epoch 8/1000\n",
            "5274/5274 [==============================] - 0s 60us/sample - loss: 0.4437 - accuracy: 0.7918 - val_loss: 0.4611 - val_accuracy: 0.7759\n",
            "Epoch 9/1000\n",
            "5274/5274 [==============================] - 0s 57us/sample - loss: 0.4454 - accuracy: 0.7958 - val_loss: 0.4387 - val_accuracy: 0.7895\n",
            "Epoch 10/1000\n",
            "5274/5274 [==============================] - 0s 59us/sample - loss: 0.4498 - accuracy: 0.7911 - val_loss: 0.4459 - val_accuracy: 0.7907\n",
            "Epoch 11/1000\n",
            "5274/5274 [==============================] - 0s 57us/sample - loss: 0.4430 - accuracy: 0.7861 - val_loss: 0.4563 - val_accuracy: 0.7850\n",
            "Epoch 12/1000\n",
            "5274/5274 [==============================] - 0s 62us/sample - loss: 0.4475 - accuracy: 0.7939 - val_loss: 0.4608 - val_accuracy: 0.7787\n",
            "Epoch 13/1000\n",
            "5274/5274 [==============================] - 0s 59us/sample - loss: 0.4443 - accuracy: 0.7907 - val_loss: 0.4722 - val_accuracy: 0.7884\n",
            "Epoch 14/1000\n",
            "5274/5274 [==============================] - 0s 60us/sample - loss: 0.4543 - accuracy: 0.7903 - val_loss: 0.4554 - val_accuracy: 0.7867\n",
            "Epoch 15/1000\n",
            "5274/5274 [==============================] - 0s 57us/sample - loss: 0.4438 - accuracy: 0.7943 - val_loss: 0.4470 - val_accuracy: 0.7929\n",
            "Epoch 16/1000\n",
            "5274/5274 [==============================] - 0s 59us/sample - loss: 0.4394 - accuracy: 0.7920 - val_loss: 0.4405 - val_accuracy: 0.7901\n",
            "Epoch 17/1000\n",
            "5274/5274 [==============================] - 0s 59us/sample - loss: 0.4398 - accuracy: 0.7950 - val_loss: 0.4500 - val_accuracy: 0.7918\n",
            "Epoch 18/1000\n",
            "5274/5274 [==============================] - 0s 60us/sample - loss: 0.4393 - accuracy: 0.7947 - val_loss: 0.4461 - val_accuracy: 0.7890\n",
            "Epoch 19/1000\n",
            "5274/5274 [==============================] - 0s 59us/sample - loss: 0.4389 - accuracy: 0.8047 - val_loss: 0.4473 - val_accuracy: 0.7873\n",
            "Epoch 20/1000\n",
            "5274/5274 [==============================] - 0s 61us/sample - loss: 0.4376 - accuracy: 0.7994 - val_loss: 0.4292 - val_accuracy: 0.8009\n",
            "Epoch 21/1000\n",
            "5274/5274 [==============================] - 0s 57us/sample - loss: 0.4345 - accuracy: 0.7981 - val_loss: 0.4328 - val_accuracy: 0.7941\n",
            "Epoch 22/1000\n",
            "5274/5274 [==============================] - 0s 65us/sample - loss: 0.4375 - accuracy: 0.7958 - val_loss: 0.4332 - val_accuracy: 0.7895\n",
            "Epoch 23/1000\n",
            "5274/5274 [==============================] - 0s 60us/sample - loss: 0.4301 - accuracy: 0.8009 - val_loss: 0.4654 - val_accuracy: 0.7912\n",
            "Epoch 24/1000\n",
            "5274/5274 [==============================] - 0s 58us/sample - loss: 0.4340 - accuracy: 0.8017 - val_loss: 0.4462 - val_accuracy: 0.7929\n",
            "Epoch 25/1000\n",
            "5274/5274 [==============================] - 0s 57us/sample - loss: 0.4425 - accuracy: 0.8003 - val_loss: 0.4380 - val_accuracy: 0.7952\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f23c08d19e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FfZRtJ7MCN3x"
      },
      "source": [
        "## Stretch Goals:\n",
        "\n",
        "- Try to implement Random Search Hyperparameter Tuning on this dataset\n",
        "- Try to implement Bayesian Optimiation tuning on this dataset using hyperas or hyperopt (if you're brave)\n",
        "- Practice hyperparameter tuning other datasets that we have looked at. How high can you get MNIST? Above 99%?\n",
        "- Study for the Sprint Challenge\n",
        " - Can you implement both perceptron and MLP models from scratch with forward and backpropagation?\n",
        " - Can you implement both perceptron and MLP models in keras and tune their hyperparameters with cross validation?"
      ]
    }
  ]
}