{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_422_Backprop_Assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "U4-S1-NN (Python3)",
      "language": "python",
      "name": "u4-s1-nn"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NGGrt9EYlCqY"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "\n",
        "# Backpropagation Practice\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 2*\n",
        "\n",
        "Using TensorFlow Keras, Implement a 3 input, 4 node hidden-layer, 1 output node Multilayer Perceptron on the following dataset:\n",
        "\n",
        "| x1 | x2 | x3 | y |\n",
        "|----|----|----|---|\n",
        "| 0  | 0  | 1  | 0 |\n",
        "| 0  | 1  | 1  | 1 |\n",
        "| 1  | 0  | 1  | 1 |\n",
        "| 0  | 1  | 0  | 1 |\n",
        "| 1  | 0  | 0  | 1 |\n",
        "| 1  | 1  | 1  | 0 |\n",
        "| 0  | 0  | 0  | 0 |\n",
        "\n",
        "If you look at the data you'll notice that the first two columns behave like an XOR gate while the last column is mostly just noise. Remember that creating an XOR gate was what the perceptron was criticized for not being able to learn.\n",
        "\n",
        "This is your \"Hello World!\" of TensorFlow.\n",
        "\n",
        "### Example TensorFlow Starter Code\n",
        "\n",
        "```python \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(3, activation='sigmoid', input_dim=2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "results = model.fit(X,y, epochs=100)\n",
        "\n",
        "```\n",
        "\n",
        "### Additional Written Tasks:\n",
        "1. Investigate the various [loss functions](https://www.tensorflow.org/api_docs/python/tf/keras/losses). Which is best suited for the task at hand (predicting 1 / 0) and why? \n",
        "2. What is the difference between a loss function and a metric? Why might we need both in Keras? \n",
        "3. Investigate the various [optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers). Stochastic Gradient Descent (`sgd`) is not the learning algorithm dejour anyone. Why is that? What do newer optimizers such as `adam` have to offer? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nEREYT-3wI1f",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "data = { 'x1': [0,0,1,0,1,1,0],\n",
        "         'x2': [0,1,0,1,0,1,0],\n",
        "         'x3': [1,1,1,0,0,1,0],\n",
        "         'y':  [0,1,1,1,1,0,0]\n",
        "       }\n",
        "\n",
        "df = pd.DataFrame.from_dict(data).astype('int')\n",
        "X = df[['x1','x2','x3']]\n",
        "y = df['y']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g74HAyv-Fnhv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "56186496-6c3a-4f35-e7b7-4aa5429eb20f"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(4, activation='relu', input_dim=3),\n",
        "    Dense(1, activation='relu')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "results = model.fit(X,y, epochs=500)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7 samples\n",
            "Epoch 1/500\n",
            "7/7 [==============================] - 0s 13ms/sample - loss: 4.7381 - acc: 0.4286\n",
            "Epoch 2/500\n",
            "7/7 [==============================] - 0s 257us/sample - loss: 4.7339 - acc: 0.4286\n",
            "Epoch 3/500\n",
            "7/7 [==============================] - 0s 232us/sample - loss: 4.7298 - acc: 0.4286\n",
            "Epoch 4/500\n",
            "7/7 [==============================] - 0s 236us/sample - loss: 4.7259 - acc: 0.4286\n",
            "Epoch 5/500\n",
            "7/7 [==============================] - 0s 254us/sample - loss: 4.7220 - acc: 0.4286\n",
            "Epoch 6/500\n",
            "7/7 [==============================] - 0s 223us/sample - loss: 4.7181 - acc: 0.4286\n",
            "Epoch 7/500\n",
            "7/7 [==============================] - 0s 230us/sample - loss: 4.7144 - acc: 0.4286\n",
            "Epoch 8/500\n",
            "7/7 [==============================] - 0s 241us/sample - loss: 4.7107 - acc: 0.4286\n",
            "Epoch 9/500\n",
            "7/7 [==============================] - 0s 247us/sample - loss: 4.7071 - acc: 0.4286\n",
            "Epoch 10/500\n",
            "7/7 [==============================] - 0s 228us/sample - loss: 4.7036 - acc: 0.4286\n",
            "Epoch 11/500\n",
            "7/7 [==============================] - 0s 608us/sample - loss: 4.7001 - acc: 0.4286\n",
            "Epoch 12/500\n",
            "7/7 [==============================] - 0s 355us/sample - loss: 4.6967 - acc: 0.4286\n",
            "Epoch 13/500\n",
            "7/7 [==============================] - 0s 187us/sample - loss: 4.6933 - acc: 0.4286\n",
            "Epoch 14/500\n",
            "7/7 [==============================] - 0s 246us/sample - loss: 4.6901 - acc: 0.4286\n",
            "Epoch 15/500\n",
            "7/7 [==============================] - 0s 185us/sample - loss: 4.6868 - acc: 0.4286\n",
            "Epoch 16/500\n",
            "7/7 [==============================] - 0s 192us/sample - loss: 4.6837 - acc: 0.4286\n",
            "Epoch 17/500\n",
            "7/7 [==============================] - 0s 190us/sample - loss: 4.6806 - acc: 0.4286\n",
            "Epoch 18/500\n",
            "7/7 [==============================] - 0s 173us/sample - loss: 4.6775 - acc: 0.4286\n",
            "Epoch 19/500\n",
            "7/7 [==============================] - 0s 186us/sample - loss: 4.6746 - acc: 0.4286\n",
            "Epoch 20/500\n",
            "7/7 [==============================] - 0s 265us/sample - loss: 4.6716 - acc: 0.4286\n",
            "Epoch 21/500\n",
            "7/7 [==============================] - 0s 387us/sample - loss: 4.6687 - acc: 0.4286\n",
            "Epoch 22/500\n",
            "7/7 [==============================] - 0s 307us/sample - loss: 4.6659 - acc: 0.4286\n",
            "Epoch 23/500\n",
            "7/7 [==============================] - 0s 280us/sample - loss: 4.6631 - acc: 0.4286\n",
            "Epoch 24/500\n",
            "7/7 [==============================] - 0s 232us/sample - loss: 4.6604 - acc: 0.4286\n",
            "Epoch 25/500\n",
            "7/7 [==============================] - 0s 190us/sample - loss: 4.6577 - acc: 0.4286\n",
            "Epoch 26/500\n",
            "7/7 [==============================] - 0s 253us/sample - loss: 4.6551 - acc: 0.4286\n",
            "Epoch 27/500\n",
            "7/7 [==============================] - 0s 218us/sample - loss: 4.6525 - acc: 0.4286\n",
            "Epoch 28/500\n",
            "7/7 [==============================] - 0s 220us/sample - loss: 4.6499 - acc: 0.4286\n",
            "Epoch 29/500\n",
            "7/7 [==============================] - 0s 156us/sample - loss: 4.6474 - acc: 0.4286\n",
            "Epoch 30/500\n",
            "7/7 [==============================] - 0s 153us/sample - loss: 4.6449 - acc: 0.4286\n",
            "Epoch 31/500\n",
            "7/7 [==============================] - 0s 166us/sample - loss: 4.6425 - acc: 0.4286\n",
            "Epoch 32/500\n",
            "7/7 [==============================] - 0s 346us/sample - loss: 4.6401 - acc: 0.4286\n",
            "Epoch 33/500\n",
            "7/7 [==============================] - 0s 228us/sample - loss: 4.6385 - acc: 0.4286\n",
            "Epoch 34/500\n",
            "7/7 [==============================] - 0s 270us/sample - loss: 4.6369 - acc: 0.4286\n",
            "Epoch 35/500\n",
            "7/7 [==============================] - 0s 266us/sample - loss: 4.6353 - acc: 0.4286\n",
            "Epoch 36/500\n",
            "7/7 [==============================] - 0s 307us/sample - loss: 4.6338 - acc: 0.4286\n",
            "Epoch 37/500\n",
            "7/7 [==============================] - 0s 313us/sample - loss: 4.6322 - acc: 0.4286\n",
            "Epoch 38/500\n",
            "7/7 [==============================] - 0s 214us/sample - loss: 4.6307 - acc: 0.4286\n",
            "Epoch 39/500\n",
            "7/7 [==============================] - 0s 196us/sample - loss: 4.6292 - acc: 0.4286\n",
            "Epoch 40/500\n",
            "7/7 [==============================] - 0s 241us/sample - loss: 4.6277 - acc: 0.4286\n",
            "Epoch 41/500\n",
            "7/7 [==============================] - 0s 350us/sample - loss: 4.6262 - acc: 0.4286\n",
            "Epoch 42/500\n",
            "7/7 [==============================] - 0s 302us/sample - loss: 4.6247 - acc: 0.5714\n",
            "Epoch 43/500\n",
            "7/7 [==============================] - 0s 195us/sample - loss: 4.6232 - acc: 0.5714\n",
            "Epoch 44/500\n",
            "7/7 [==============================] - 0s 308us/sample - loss: 4.6218 - acc: 0.5714\n",
            "Epoch 45/500\n",
            "7/7 [==============================] - 0s 619us/sample - loss: 4.6203 - acc: 0.5714\n",
            "Epoch 46/500\n",
            "7/7 [==============================] - 0s 299us/sample - loss: 4.6189 - acc: 0.5714\n",
            "Epoch 47/500\n",
            "7/7 [==============================] - 0s 407us/sample - loss: 4.6175 - acc: 0.5714\n",
            "Epoch 48/500\n",
            "7/7 [==============================] - 0s 241us/sample - loss: 4.6161 - acc: 0.5714\n",
            "Epoch 49/500\n",
            "7/7 [==============================] - 0s 266us/sample - loss: 4.6147 - acc: 0.5714\n",
            "Epoch 50/500\n",
            "7/7 [==============================] - 0s 264us/sample - loss: 4.6133 - acc: 0.5714\n",
            "Epoch 51/500\n",
            "7/7 [==============================] - 0s 252us/sample - loss: 4.6119 - acc: 0.5714\n",
            "Epoch 52/500\n",
            "7/7 [==============================] - 0s 583us/sample - loss: 4.6105 - acc: 0.5714\n",
            "Epoch 53/500\n",
            "7/7 [==============================] - 0s 373us/sample - loss: 4.6092 - acc: 0.5714\n",
            "Epoch 54/500\n",
            "7/7 [==============================] - 0s 175us/sample - loss: 4.6078 - acc: 0.5714\n",
            "Epoch 55/500\n",
            "7/7 [==============================] - 0s 152us/sample - loss: 4.6065 - acc: 0.5714\n",
            "Epoch 56/500\n",
            "7/7 [==============================] - 0s 267us/sample - loss: 4.6052 - acc: 0.5714\n",
            "Epoch 57/500\n",
            "7/7 [==============================] - 0s 191us/sample - loss: 4.6039 - acc: 0.7143\n",
            "Epoch 58/500\n",
            "7/7 [==============================] - 0s 369us/sample - loss: 4.6026 - acc: 0.7143\n",
            "Epoch 59/500\n",
            "7/7 [==============================] - 0s 256us/sample - loss: 4.6013 - acc: 0.7143\n",
            "Epoch 60/500\n",
            "7/7 [==============================] - 0s 256us/sample - loss: 4.6000 - acc: 0.7143\n",
            "Epoch 61/500\n",
            "7/7 [==============================] - 0s 177us/sample - loss: 4.5987 - acc: 0.7143\n",
            "Epoch 62/500\n",
            "7/7 [==============================] - 0s 307us/sample - loss: 4.5974 - acc: 0.7143\n",
            "Epoch 63/500\n",
            "7/7 [==============================] - 0s 310us/sample - loss: 4.5962 - acc: 0.7143\n",
            "Epoch 64/500\n",
            "7/7 [==============================] - 0s 276us/sample - loss: 4.5949 - acc: 0.7143\n",
            "Epoch 65/500\n",
            "7/7 [==============================] - 0s 241us/sample - loss: 4.5937 - acc: 0.7143\n",
            "Epoch 66/500\n",
            "7/7 [==============================] - 0s 264us/sample - loss: 4.5924 - acc: 0.7143\n",
            "Epoch 67/500\n",
            "7/7 [==============================] - 0s 197us/sample - loss: 4.5912 - acc: 0.7143\n",
            "Epoch 68/500\n",
            "7/7 [==============================] - 0s 279us/sample - loss: 4.5900 - acc: 0.7143\n",
            "Epoch 69/500\n",
            "7/7 [==============================] - 0s 259us/sample - loss: 4.5888 - acc: 0.7143\n",
            "Epoch 70/500\n",
            "7/7 [==============================] - 0s 255us/sample - loss: 4.5876 - acc: 0.7143\n",
            "Epoch 71/500\n",
            "7/7 [==============================] - 0s 298us/sample - loss: 4.5864 - acc: 0.7143\n",
            "Epoch 72/500\n",
            "7/7 [==============================] - 0s 265us/sample - loss: 4.5852 - acc: 0.7143\n",
            "Epoch 73/500\n",
            "7/7 [==============================] - 0s 300us/sample - loss: 4.5841 - acc: 0.7143\n",
            "Epoch 74/500\n",
            "7/7 [==============================] - 0s 279us/sample - loss: 4.5829 - acc: 0.7143\n",
            "Epoch 75/500\n",
            "7/7 [==============================] - 0s 277us/sample - loss: 4.5817 - acc: 0.7143\n",
            "Epoch 76/500\n",
            "7/7 [==============================] - 0s 375us/sample - loss: 4.5806 - acc: 0.7143\n",
            "Epoch 77/500\n",
            "7/7 [==============================] - 0s 247us/sample - loss: 4.5794 - acc: 0.7143\n",
            "Epoch 78/500\n",
            "7/7 [==============================] - 0s 252us/sample - loss: 4.5783 - acc: 0.7143\n",
            "Epoch 79/500\n",
            "7/7 [==============================] - 0s 307us/sample - loss: 4.5772 - acc: 0.7143\n",
            "Epoch 80/500\n",
            "7/7 [==============================] - 0s 551us/sample - loss: 4.5760 - acc: 0.7143\n",
            "Epoch 81/500\n",
            "7/7 [==============================] - 0s 171us/sample - loss: 4.5749 - acc: 0.7143\n",
            "Epoch 82/500\n",
            "7/7 [==============================] - 0s 243us/sample - loss: 4.5738 - acc: 0.7143\n",
            "Epoch 83/500\n",
            "7/7 [==============================] - 0s 257us/sample - loss: 4.5727 - acc: 0.7143\n",
            "Epoch 84/500\n",
            "7/7 [==============================] - 0s 178us/sample - loss: 4.5716 - acc: 0.7143\n",
            "Epoch 85/500\n",
            "7/7 [==============================] - 0s 193us/sample - loss: 4.5706 - acc: 0.7143\n",
            "Epoch 86/500\n",
            "7/7 [==============================] - 0s 365us/sample - loss: 4.5695 - acc: 0.7143\n",
            "Epoch 87/500\n",
            "7/7 [==============================] - 0s 235us/sample - loss: 4.5684 - acc: 0.7143\n",
            "Epoch 88/500\n",
            "7/7 [==============================] - 0s 242us/sample - loss: 4.5673 - acc: 0.7143\n",
            "Epoch 89/500\n",
            "7/7 [==============================] - 0s 319us/sample - loss: 4.5663 - acc: 0.7143\n",
            "Epoch 90/500\n",
            "7/7 [==============================] - 0s 199us/sample - loss: 4.5652 - acc: 0.7143\n",
            "Epoch 91/500\n",
            "7/7 [==============================] - 0s 297us/sample - loss: 4.5642 - acc: 0.7143\n",
            "Epoch 92/500\n",
            "7/7 [==============================] - 0s 244us/sample - loss: 4.5631 - acc: 0.7143\n",
            "Epoch 93/500\n",
            "7/7 [==============================] - 0s 281us/sample - loss: 4.5621 - acc: 0.7143\n",
            "Epoch 94/500\n",
            "7/7 [==============================] - 0s 224us/sample - loss: 4.5611 - acc: 0.7143\n",
            "Epoch 95/500\n",
            "7/7 [==============================] - 0s 226us/sample - loss: 4.5601 - acc: 0.7143\n",
            "Epoch 96/500\n",
            "7/7 [==============================] - 0s 277us/sample - loss: 4.5591 - acc: 0.7143\n",
            "Epoch 97/500\n",
            "7/7 [==============================] - 0s 218us/sample - loss: 4.5580 - acc: 0.7143\n",
            "Epoch 98/500\n",
            "7/7 [==============================] - 0s 201us/sample - loss: 4.5570 - acc: 0.7143\n",
            "Epoch 99/500\n",
            "7/7 [==============================] - 0s 487us/sample - loss: 4.5560 - acc: 0.7143\n",
            "Epoch 100/500\n",
            "7/7 [==============================] - 0s 220us/sample - loss: 4.5551 - acc: 0.7143\n",
            "Epoch 101/500\n",
            "7/7 [==============================] - 0s 358us/sample - loss: 4.5541 - acc: 0.7143\n",
            "Epoch 102/500\n",
            "7/7 [==============================] - 0s 277us/sample - loss: 4.5531 - acc: 0.7143\n",
            "Epoch 103/500\n",
            "7/7 [==============================] - 0s 249us/sample - loss: 4.5521 - acc: 0.7143\n",
            "Epoch 104/500\n",
            "7/7 [==============================] - 0s 200us/sample - loss: 4.5512 - acc: 0.7143\n",
            "Epoch 105/500\n",
            "7/7 [==============================] - 0s 223us/sample - loss: 4.5502 - acc: 0.7143\n",
            "Epoch 106/500\n",
            "7/7 [==============================] - 0s 199us/sample - loss: 4.5492 - acc: 0.7143\n",
            "Epoch 107/500\n",
            "7/7 [==============================] - 0s 252us/sample - loss: 4.5483 - acc: 0.7143\n",
            "Epoch 108/500\n",
            "7/7 [==============================] - 0s 305us/sample - loss: 4.5474 - acc: 0.7143\n",
            "Epoch 109/500\n",
            "7/7 [==============================] - 0s 180us/sample - loss: 4.5464 - acc: 0.7143\n",
            "Epoch 110/500\n",
            "7/7 [==============================] - 0s 340us/sample - loss: 4.5455 - acc: 0.7143\n",
            "Epoch 111/500\n",
            "7/7 [==============================] - 0s 275us/sample - loss: 4.5446 - acc: 0.7143\n",
            "Epoch 112/500\n",
            "7/7 [==============================] - 0s 250us/sample - loss: 4.5436 - acc: 0.7143\n",
            "Epoch 113/500\n",
            "7/7 [==============================] - 0s 202us/sample - loss: 4.5427 - acc: 0.7143\n",
            "Epoch 114/500\n",
            "7/7 [==============================] - 0s 303us/sample - loss: 4.5418 - acc: 0.7143\n",
            "Epoch 115/500\n",
            "7/7 [==============================] - 0s 283us/sample - loss: 4.5409 - acc: 0.7143\n",
            "Epoch 116/500\n",
            "7/7 [==============================] - 0s 282us/sample - loss: 4.5400 - acc: 0.7143\n",
            "Epoch 117/500\n",
            "7/7 [==============================] - 0s 283us/sample - loss: 4.5391 - acc: 0.7143\n",
            "Epoch 118/500\n",
            "7/7 [==============================] - 0s 318us/sample - loss: 4.5382 - acc: 0.7143\n",
            "Epoch 119/500\n",
            "7/7 [==============================] - 0s 231us/sample - loss: 4.5373 - acc: 0.7143\n",
            "Epoch 120/500\n",
            "7/7 [==============================] - 0s 380us/sample - loss: 4.5364 - acc: 0.7143\n",
            "Epoch 121/500\n",
            "7/7 [==============================] - 0s 722us/sample - loss: 4.5356 - acc: 0.7143\n",
            "Epoch 122/500\n",
            "7/7 [==============================] - 0s 270us/sample - loss: 4.5347 - acc: 0.7143\n",
            "Epoch 123/500\n",
            "7/7 [==============================] - 0s 313us/sample - loss: 4.5338 - acc: 0.7143\n",
            "Epoch 124/500\n",
            "7/7 [==============================] - 0s 326us/sample - loss: 4.5330 - acc: 0.7143\n",
            "Epoch 125/500\n",
            "7/7 [==============================] - 0s 153us/sample - loss: 4.5321 - acc: 0.7143\n",
            "Epoch 126/500\n",
            "7/7 [==============================] - 0s 322us/sample - loss: 4.5312 - acc: 0.7143\n",
            "Epoch 127/500\n",
            "7/7 [==============================] - 0s 159us/sample - loss: 4.5306 - acc: 0.7143\n",
            "Epoch 128/500\n",
            "7/7 [==============================] - 0s 238us/sample - loss: 4.5300 - acc: 0.7143\n",
            "Epoch 129/500\n",
            "7/7 [==============================] - 0s 397us/sample - loss: 4.5294 - acc: 0.7143\n",
            "Epoch 130/500\n",
            "7/7 [==============================] - 0s 262us/sample - loss: 4.5287 - acc: 0.7143\n",
            "Epoch 131/500\n",
            "7/7 [==============================] - 0s 387us/sample - loss: 4.5281 - acc: 0.7143\n",
            "Epoch 132/500\n",
            "7/7 [==============================] - 0s 387us/sample - loss: 4.5274 - acc: 0.7143\n",
            "Epoch 133/500\n",
            "7/7 [==============================] - 0s 398us/sample - loss: 4.5267 - acc: 0.7143\n",
            "Epoch 134/500\n",
            "7/7 [==============================] - 0s 330us/sample - loss: 4.5260 - acc: 0.7143\n",
            "Epoch 135/500\n",
            "7/7 [==============================] - 0s 230us/sample - loss: 4.5253 - acc: 0.7143\n",
            "Epoch 136/500\n",
            "7/7 [==============================] - 0s 250us/sample - loss: 4.5246 - acc: 0.7143\n",
            "Epoch 137/500\n",
            "7/7 [==============================] - 0s 169us/sample - loss: 4.5239 - acc: 0.7143\n",
            "Epoch 138/500\n",
            "7/7 [==============================] - 0s 356us/sample - loss: 4.5232 - acc: 0.7143\n",
            "Epoch 139/500\n",
            "7/7 [==============================] - 0s 289us/sample - loss: 4.5225 - acc: 0.7143\n",
            "Epoch 140/500\n",
            "7/7 [==============================] - 0s 264us/sample - loss: 4.5218 - acc: 0.7143\n",
            "Epoch 141/500\n",
            "7/7 [==============================] - 0s 246us/sample - loss: 4.5210 - acc: 0.7143\n",
            "Epoch 142/500\n",
            "7/7 [==============================] - 0s 319us/sample - loss: 4.5203 - acc: 0.7143\n",
            "Epoch 143/500\n",
            "7/7 [==============================] - 0s 274us/sample - loss: 4.5196 - acc: 0.7143\n",
            "Epoch 144/500\n",
            "7/7 [==============================] - 0s 277us/sample - loss: 4.5188 - acc: 0.7143\n",
            "Epoch 145/500\n",
            "7/7 [==============================] - 0s 448us/sample - loss: 4.5182 - acc: 0.7143\n",
            "Epoch 146/500\n",
            "7/7 [==============================] - 0s 396us/sample - loss: 4.5176 - acc: 0.7143\n",
            "Epoch 147/500\n",
            "7/7 [==============================] - 0s 161us/sample - loss: 4.5169 - acc: 0.7143\n",
            "Epoch 148/500\n",
            "7/7 [==============================] - 0s 177us/sample - loss: 4.5163 - acc: 0.7143\n",
            "Epoch 149/500\n",
            "7/7 [==============================] - 0s 232us/sample - loss: 4.5156 - acc: 0.7143\n",
            "Epoch 150/500\n",
            "7/7 [==============================] - 0s 193us/sample - loss: 4.5150 - acc: 0.7143\n",
            "Epoch 151/500\n",
            "7/7 [==============================] - 0s 644us/sample - loss: 4.5143 - acc: 0.7143\n",
            "Epoch 152/500\n",
            "7/7 [==============================] - 0s 358us/sample - loss: 4.5136 - acc: 0.7143\n",
            "Epoch 153/500\n",
            "7/7 [==============================] - 0s 239us/sample - loss: 4.5129 - acc: 0.7143\n",
            "Epoch 154/500\n",
            "7/7 [==============================] - 0s 257us/sample - loss: 4.5122 - acc: 0.7143\n",
            "Epoch 155/500\n",
            "7/7 [==============================] - 0s 290us/sample - loss: 4.5116 - acc: 0.7143\n",
            "Epoch 156/500\n",
            "7/7 [==============================] - 0s 247us/sample - loss: 4.5110 - acc: 0.7143\n",
            "Epoch 157/500\n",
            "7/7 [==============================] - 0s 195us/sample - loss: 4.5104 - acc: 0.7143\n",
            "Epoch 158/500\n",
            "7/7 [==============================] - 0s 232us/sample - loss: 4.5098 - acc: 0.7143\n",
            "Epoch 159/500\n",
            "7/7 [==============================] - 0s 302us/sample - loss: 4.5091 - acc: 0.7143\n",
            "Epoch 160/500\n",
            "7/7 [==============================] - 0s 321us/sample - loss: 4.5086 - acc: 0.7143\n",
            "Epoch 161/500\n",
            "7/7 [==============================] - 0s 369us/sample - loss: 4.5081 - acc: 0.7143\n",
            "Epoch 162/500\n",
            "7/7 [==============================] - 0s 322us/sample - loss: 4.5076 - acc: 0.7143\n",
            "Epoch 163/500\n",
            "7/7 [==============================] - 0s 303us/sample - loss: 4.5070 - acc: 0.7143\n",
            "Epoch 164/500\n",
            "7/7 [==============================] - 0s 241us/sample - loss: 4.5065 - acc: 0.7143\n",
            "Epoch 165/500\n",
            "7/7 [==============================] - 0s 298us/sample - loss: 4.5059 - acc: 0.7143\n",
            "Epoch 166/500\n",
            "7/7 [==============================] - 0s 255us/sample - loss: 4.5053 - acc: 0.7143\n",
            "Epoch 167/500\n",
            "7/7 [==============================] - 0s 267us/sample - loss: 4.5047 - acc: 0.7143\n",
            "Epoch 168/500\n",
            "7/7 [==============================] - 0s 245us/sample - loss: 4.5041 - acc: 0.7143\n",
            "Epoch 169/500\n",
            "7/7 [==============================] - 0s 257us/sample - loss: 4.5035 - acc: 0.7143\n",
            "Epoch 170/500\n",
            "7/7 [==============================] - 0s 265us/sample - loss: 4.5029 - acc: 0.7143\n",
            "Epoch 171/500\n",
            "7/7 [==============================] - 0s 276us/sample - loss: 4.5023 - acc: 0.7143\n",
            "Epoch 172/500\n",
            "7/7 [==============================] - 0s 273us/sample - loss: 4.5017 - acc: 0.7143\n",
            "Epoch 173/500\n",
            "7/7 [==============================] - 0s 322us/sample - loss: 4.5011 - acc: 0.7143\n",
            "Epoch 174/500\n",
            "7/7 [==============================] - 0s 264us/sample - loss: 4.5006 - acc: 0.7143\n",
            "Epoch 175/500\n",
            "7/7 [==============================] - 0s 300us/sample - loss: 4.5000 - acc: 0.7143\n",
            "Epoch 176/500\n",
            "7/7 [==============================] - 0s 265us/sample - loss: 4.4995 - acc: 0.7143\n",
            "Epoch 177/500\n",
            "7/7 [==============================] - 0s 255us/sample - loss: 4.4990 - acc: 0.7143\n",
            "Epoch 178/500\n",
            "7/7 [==============================] - 0s 231us/sample - loss: 4.4984 - acc: 0.7143\n",
            "Epoch 179/500\n",
            "7/7 [==============================] - 0s 308us/sample - loss: 4.4978 - acc: 0.7143\n",
            "Epoch 180/500\n",
            "7/7 [==============================] - 0s 335us/sample - loss: 4.4973 - acc: 0.7143\n",
            "Epoch 181/500\n",
            "7/7 [==============================] - 0s 312us/sample - loss: 4.4967 - acc: 0.7143\n",
            "Epoch 182/500\n",
            "7/7 [==============================] - 0s 310us/sample - loss: 4.4962 - acc: 0.7143\n",
            "Epoch 183/500\n",
            "7/7 [==============================] - 0s 261us/sample - loss: 4.4956 - acc: 0.7143\n",
            "Epoch 184/500\n",
            "7/7 [==============================] - 0s 312us/sample - loss: 4.4951 - acc: 0.7143\n",
            "Epoch 185/500\n",
            "7/7 [==============================] - 0s 284us/sample - loss: 4.4945 - acc: 0.7143\n",
            "Epoch 186/500\n",
            "7/7 [==============================] - 0s 283us/sample - loss: 4.4940 - acc: 0.7143\n",
            "Epoch 187/500\n",
            "7/7 [==============================] - 0s 282us/sample - loss: 4.4934 - acc: 0.7143\n",
            "Epoch 188/500\n",
            "7/7 [==============================] - 0s 246us/sample - loss: 4.4929 - acc: 0.7143\n",
            "Epoch 189/500\n",
            "7/7 [==============================] - 0s 340us/sample - loss: 4.4924 - acc: 0.7143\n",
            "Epoch 190/500\n",
            "7/7 [==============================] - 0s 326us/sample - loss: 4.4918 - acc: 0.7143\n",
            "Epoch 191/500\n",
            "7/7 [==============================] - 0s 313us/sample - loss: 4.4913 - acc: 0.7143\n",
            "Epoch 192/500\n",
            "7/7 [==============================] - 0s 260us/sample - loss: 4.4907 - acc: 0.7143\n",
            "Epoch 193/500\n",
            "7/7 [==============================] - 0s 273us/sample - loss: 4.4902 - acc: 0.7143\n",
            "Epoch 194/500\n",
            "7/7 [==============================] - 0s 300us/sample - loss: 4.4896 - acc: 0.7143\n",
            "Epoch 195/500\n",
            "7/7 [==============================] - 0s 249us/sample - loss: 4.4891 - acc: 0.7143\n",
            "Epoch 196/500\n",
            "7/7 [==============================] - 0s 234us/sample - loss: 4.4886 - acc: 0.7143\n",
            "Epoch 197/500\n",
            "7/7 [==============================] - 0s 262us/sample - loss: 4.4881 - acc: 0.7143\n",
            "Epoch 198/500\n",
            "7/7 [==============================] - 0s 265us/sample - loss: 4.4875 - acc: 0.7143\n",
            "Epoch 199/500\n",
            "7/7 [==============================] - 0s 267us/sample - loss: 4.4870 - acc: 0.7143\n",
            "Epoch 200/500\n",
            "7/7 [==============================] - 0s 271us/sample - loss: 4.4865 - acc: 0.7143\n",
            "Epoch 201/500\n",
            "7/7 [==============================] - 0s 268us/sample - loss: 4.4859 - acc: 0.7143\n",
            "Epoch 202/500\n",
            "7/7 [==============================] - 0s 272us/sample - loss: 4.4854 - acc: 0.7143\n",
            "Epoch 203/500\n",
            "7/7 [==============================] - 0s 264us/sample - loss: 4.4849 - acc: 0.7143\n",
            "Epoch 204/500\n",
            "7/7 [==============================] - 0s 411us/sample - loss: 4.4844 - acc: 0.7143\n",
            "Epoch 205/500\n",
            "7/7 [==============================] - 0s 348us/sample - loss: 4.4838 - acc: 0.7143\n",
            "Epoch 206/500\n",
            "7/7 [==============================] - 0s 334us/sample - loss: 4.4833 - acc: 0.7143\n",
            "Epoch 207/500\n",
            "7/7 [==============================] - 0s 292us/sample - loss: 4.4828 - acc: 0.7143\n",
            "Epoch 208/500\n",
            "7/7 [==============================] - 0s 286us/sample - loss: 4.4823 - acc: 0.7143\n",
            "Epoch 209/500\n",
            "7/7 [==============================] - 0s 240us/sample - loss: 4.4818 - acc: 0.7143\n",
            "Epoch 210/500\n",
            "7/7 [==============================] - 0s 276us/sample - loss: 4.4812 - acc: 0.7143\n",
            "Epoch 211/500\n",
            "7/7 [==============================] - 0s 266us/sample - loss: 4.4808 - acc: 0.7143\n",
            "Epoch 212/500\n",
            "7/7 [==============================] - 0s 275us/sample - loss: 4.4803 - acc: 0.7143\n",
            "Epoch 213/500\n",
            "7/7 [==============================] - 0s 286us/sample - loss: 4.4797 - acc: 0.7143\n",
            "Epoch 214/500\n",
            "7/7 [==============================] - 0s 238us/sample - loss: 4.4792 - acc: 0.7143\n",
            "Epoch 215/500\n",
            "7/7 [==============================] - 0s 244us/sample - loss: 4.4787 - acc: 0.7143\n",
            "Epoch 216/500\n",
            "7/7 [==============================] - 0s 310us/sample - loss: 4.4782 - acc: 0.7143\n",
            "Epoch 217/500\n",
            "7/7 [==============================] - 0s 244us/sample - loss: 4.4777 - acc: 0.7143\n",
            "Epoch 218/500\n",
            "7/7 [==============================] - 0s 265us/sample - loss: 4.4772 - acc: 0.7143\n",
            "Epoch 219/500\n",
            "7/7 [==============================] - 0s 267us/sample - loss: 4.4767 - acc: 0.7143\n",
            "Epoch 220/500\n",
            "7/7 [==============================] - 0s 264us/sample - loss: 4.4762 - acc: 0.7143\n",
            "Epoch 221/500\n",
            "7/7 [==============================] - 0s 258us/sample - loss: 4.4757 - acc: 0.7143\n",
            "Epoch 222/500\n",
            "7/7 [==============================] - 0s 306us/sample - loss: 4.4752 - acc: 0.7143\n",
            "Epoch 223/500\n",
            "7/7 [==============================] - 0s 258us/sample - loss: 4.4747 - acc: 0.7143\n",
            "Epoch 224/500\n",
            "7/7 [==============================] - 0s 282us/sample - loss: 4.4742 - acc: 0.7143\n",
            "Epoch 225/500\n",
            "7/7 [==============================] - 0s 266us/sample - loss: 4.4737 - acc: 0.7143\n",
            "Epoch 226/500\n",
            "7/7 [==============================] - 0s 274us/sample - loss: 4.4731 - acc: 0.7143\n",
            "Epoch 227/500\n",
            "7/7 [==============================] - 0s 323us/sample - loss: 4.4727 - acc: 0.7143\n",
            "Epoch 228/500\n",
            "7/7 [==============================] - 0s 330us/sample - loss: 4.4722 - acc: 0.7143\n",
            "Epoch 229/500\n",
            "7/7 [==============================] - 0s 264us/sample - loss: 4.4717 - acc: 0.7143\n",
            "Epoch 230/500\n",
            "7/7 [==============================] - 0s 329us/sample - loss: 4.4712 - acc: 0.7143\n",
            "Epoch 231/500\n",
            "7/7 [==============================] - 0s 300us/sample - loss: 4.4707 - acc: 0.7143\n",
            "Epoch 232/500\n",
            "7/7 [==============================] - 0s 343us/sample - loss: 4.4702 - acc: 0.7143\n",
            "Epoch 233/500\n",
            "7/7 [==============================] - 0s 309us/sample - loss: 4.4697 - acc: 0.7143\n",
            "Epoch 234/500\n",
            "7/7 [==============================] - 0s 243us/sample - loss: 4.4692 - acc: 0.7143\n",
            "Epoch 235/500\n",
            "7/7 [==============================] - 0s 274us/sample - loss: 4.4687 - acc: 0.7143\n",
            "Epoch 236/500\n",
            "7/7 [==============================] - 0s 266us/sample - loss: 4.4682 - acc: 0.7143\n",
            "Epoch 237/500\n",
            "7/7 [==============================] - 0s 264us/sample - loss: 4.4677 - acc: 0.7143\n",
            "Epoch 238/500\n",
            "7/7 [==============================] - 0s 230us/sample - loss: 4.4672 - acc: 0.7143\n",
            "Epoch 239/500\n",
            "7/7 [==============================] - 0s 275us/sample - loss: 4.4667 - acc: 0.7143\n",
            "Epoch 240/500\n",
            "7/7 [==============================] - 0s 250us/sample - loss: 4.4662 - acc: 0.7143\n",
            "Epoch 241/500\n",
            "7/7 [==============================] - 0s 312us/sample - loss: 4.4658 - acc: 0.7143\n",
            "Epoch 242/500\n",
            "7/7 [==============================] - 0s 261us/sample - loss: 4.4654 - acc: 0.7143\n",
            "Epoch 243/500\n",
            "7/7 [==============================] - 0s 387us/sample - loss: 4.4652 - acc: 0.7143\n",
            "Epoch 244/500\n",
            "7/7 [==============================] - 0s 327us/sample - loss: 4.4650 - acc: 0.7143\n",
            "Epoch 245/500\n",
            "7/7 [==============================] - 0s 307us/sample - loss: 4.4648 - acc: 0.7143\n",
            "Epoch 246/500\n",
            "7/7 [==============================] - 0s 425us/sample - loss: 4.4647 - acc: 0.7143\n",
            "Epoch 247/500\n",
            "7/7 [==============================] - 0s 302us/sample - loss: 4.4645 - acc: 0.7143\n",
            "Epoch 248/500\n",
            "7/7 [==============================] - 0s 249us/sample - loss: 4.4644 - acc: 0.7143\n",
            "Epoch 249/500\n",
            "7/7 [==============================] - 0s 435us/sample - loss: 4.4642 - acc: 0.7143\n",
            "Epoch 250/500\n",
            "7/7 [==============================] - 0s 294us/sample - loss: 4.4641 - acc: 0.7143\n",
            "Epoch 251/500\n",
            "7/7 [==============================] - 0s 309us/sample - loss: 4.4639 - acc: 0.7143\n",
            "Epoch 252/500\n",
            "7/7 [==============================] - 0s 365us/sample - loss: 4.4638 - acc: 0.7143\n",
            "Epoch 253/500\n",
            "7/7 [==============================] - 0s 253us/sample - loss: 4.4636 - acc: 0.7143\n",
            "Epoch 254/500\n",
            "7/7 [==============================] - 0s 277us/sample - loss: 4.4635 - acc: 0.7143\n",
            "Epoch 255/500\n",
            "7/7 [==============================] - 0s 305us/sample - loss: 4.4634 - acc: 0.7143\n",
            "Epoch 256/500\n",
            "7/7 [==============================] - 0s 251us/sample - loss: 4.4633 - acc: 0.7143\n",
            "Epoch 257/500\n",
            "7/7 [==============================] - 0s 316us/sample - loss: 4.4631 - acc: 0.7143\n",
            "Epoch 258/500\n",
            "7/7 [==============================] - 0s 395us/sample - loss: 4.4630 - acc: 0.7143\n",
            "Epoch 259/500\n",
            "7/7 [==============================] - 0s 353us/sample - loss: 4.4629 - acc: 0.7143\n",
            "Epoch 260/500\n",
            "7/7 [==============================] - 0s 303us/sample - loss: 4.4628 - acc: 0.7143\n",
            "Epoch 261/500\n",
            "7/7 [==============================] - 0s 286us/sample - loss: 4.4626 - acc: 0.7143\n",
            "Epoch 262/500\n",
            "7/7 [==============================] - 0s 262us/sample - loss: 4.4625 - acc: 0.7143\n",
            "Epoch 263/500\n",
            "7/7 [==============================] - 0s 266us/sample - loss: 4.4624 - acc: 0.7143\n",
            "Epoch 264/500\n",
            "7/7 [==============================] - 0s 303us/sample - loss: 4.4623 - acc: 0.7143\n",
            "Epoch 265/500\n",
            "7/7 [==============================] - 0s 304us/sample - loss: 4.4622 - acc: 0.7143\n",
            "Epoch 266/500\n",
            "7/7 [==============================] - 0s 355us/sample - loss: 4.4620 - acc: 0.7143\n",
            "Epoch 267/500\n",
            "7/7 [==============================] - 0s 340us/sample - loss: 4.4619 - acc: 0.7143\n",
            "Epoch 268/500\n",
            "7/7 [==============================] - 0s 341us/sample - loss: 4.4618 - acc: 0.7143\n",
            "Epoch 269/500\n",
            "7/7 [==============================] - 0s 262us/sample - loss: 4.4617 - acc: 0.7143\n",
            "Epoch 270/500\n",
            "7/7 [==============================] - 0s 376us/sample - loss: 4.4616 - acc: 0.7143\n",
            "Epoch 271/500\n",
            "7/7 [==============================] - 0s 478us/sample - loss: 4.4615 - acc: 0.7143\n",
            "Epoch 272/500\n",
            "7/7 [==============================] - 0s 408us/sample - loss: 4.4614 - acc: 0.7143\n",
            "Epoch 273/500\n",
            "7/7 [==============================] - 0s 406us/sample - loss: 4.4612 - acc: 0.7143\n",
            "Epoch 274/500\n",
            "7/7 [==============================] - 0s 515us/sample - loss: 4.4611 - acc: 0.7143\n",
            "Epoch 275/500\n",
            "7/7 [==============================] - 0s 439us/sample - loss: 4.4610 - acc: 0.7143\n",
            "Epoch 276/500\n",
            "7/7 [==============================] - 0s 467us/sample - loss: 4.4609 - acc: 0.7143\n",
            "Epoch 277/500\n",
            "7/7 [==============================] - 0s 357us/sample - loss: 4.4608 - acc: 0.7143\n",
            "Epoch 278/500\n",
            "7/7 [==============================] - 0s 496us/sample - loss: 4.4607 - acc: 0.7143\n",
            "Epoch 279/500\n",
            "7/7 [==============================] - 0s 475us/sample - loss: 4.4606 - acc: 0.7143\n",
            "Epoch 280/500\n",
            "7/7 [==============================] - 0s 429us/sample - loss: 4.4604 - acc: 0.7143\n",
            "Epoch 281/500\n",
            "7/7 [==============================] - 0s 452us/sample - loss: 4.4603 - acc: 0.7143\n",
            "Epoch 282/500\n",
            "7/7 [==============================] - 0s 296us/sample - loss: 4.4602 - acc: 0.7143\n",
            "Epoch 283/500\n",
            "7/7 [==============================] - 0s 613us/sample - loss: 4.4601 - acc: 0.7143\n",
            "Epoch 284/500\n",
            "7/7 [==============================] - 0s 780us/sample - loss: 4.4600 - acc: 0.7143\n",
            "Epoch 285/500\n",
            "7/7 [==============================] - 0s 552us/sample - loss: 4.4599 - acc: 0.7143\n",
            "Epoch 286/500\n",
            "7/7 [==============================] - 0s 682us/sample - loss: 4.4598 - acc: 0.7143\n",
            "Epoch 287/500\n",
            "7/7 [==============================] - 0s 662us/sample - loss: 4.4597 - acc: 0.7143\n",
            "Epoch 288/500\n",
            "7/7 [==============================] - 0s 444us/sample - loss: 4.4595 - acc: 0.7143\n",
            "Epoch 289/500\n",
            "7/7 [==============================] - 0s 430us/sample - loss: 4.4594 - acc: 0.7143\n",
            "Epoch 290/500\n",
            "7/7 [==============================] - 0s 420us/sample - loss: 4.4593 - acc: 0.7143\n",
            "Epoch 291/500\n",
            "7/7 [==============================] - 0s 399us/sample - loss: 4.4592 - acc: 0.7143\n",
            "Epoch 292/500\n",
            "7/7 [==============================] - 0s 423us/sample - loss: 4.4591 - acc: 0.7143\n",
            "Epoch 293/500\n",
            "7/7 [==============================] - 0s 391us/sample - loss: 4.4590 - acc: 0.7143\n",
            "Epoch 294/500\n",
            "7/7 [==============================] - 0s 433us/sample - loss: 4.4588 - acc: 0.7143\n",
            "Epoch 295/500\n",
            "7/7 [==============================] - 0s 255us/sample - loss: 4.4587 - acc: 0.7143\n",
            "Epoch 296/500\n",
            "7/7 [==============================] - 0s 285us/sample - loss: 4.4586 - acc: 0.7143\n",
            "Epoch 297/500\n",
            "7/7 [==============================] - 0s 309us/sample - loss: 4.4585 - acc: 0.7143\n",
            "Epoch 298/500\n",
            "7/7 [==============================] - 0s 456us/sample - loss: 4.4584 - acc: 0.7143\n",
            "Epoch 299/500\n",
            "7/7 [==============================] - 0s 437us/sample - loss: 4.4583 - acc: 0.7143\n",
            "Epoch 300/500\n",
            "7/7 [==============================] - 0s 309us/sample - loss: 4.4582 - acc: 0.7143\n",
            "Epoch 301/500\n",
            "7/7 [==============================] - 0s 254us/sample - loss: 4.4580 - acc: 0.7143\n",
            "Epoch 302/500\n",
            "7/7 [==============================] - 0s 432us/sample - loss: 4.4579 - acc: 0.7143\n",
            "Epoch 303/500\n",
            "7/7 [==============================] - 0s 439us/sample - loss: 4.4578 - acc: 0.7143\n",
            "Epoch 304/500\n",
            "7/7 [==============================] - 0s 354us/sample - loss: 4.4577 - acc: 0.7143\n",
            "Epoch 305/500\n",
            "7/7 [==============================] - 0s 377us/sample - loss: 4.4576 - acc: 0.7143\n",
            "Epoch 306/500\n",
            "7/7 [==============================] - 0s 283us/sample - loss: 4.4575 - acc: 0.7143\n",
            "Epoch 307/500\n",
            "7/7 [==============================] - 0s 539us/sample - loss: 4.4573 - acc: 0.7143\n",
            "Epoch 308/500\n",
            "7/7 [==============================] - 0s 414us/sample - loss: 4.4572 - acc: 0.7143\n",
            "Epoch 309/500\n",
            "7/7 [==============================] - 0s 343us/sample - loss: 4.4571 - acc: 0.7143\n",
            "Epoch 310/500\n",
            "7/7 [==============================] - 0s 352us/sample - loss: 4.4570 - acc: 0.7143\n",
            "Epoch 311/500\n",
            "7/7 [==============================] - 0s 348us/sample - loss: 4.4569 - acc: 0.7143\n",
            "Epoch 312/500\n",
            "7/7 [==============================] - 0s 350us/sample - loss: 4.4568 - acc: 0.7143\n",
            "Epoch 313/500\n",
            "7/7 [==============================] - 0s 427us/sample - loss: 4.4566 - acc: 0.7143\n",
            "Epoch 314/500\n",
            "7/7 [==============================] - 0s 376us/sample - loss: 4.4565 - acc: 0.7143\n",
            "Epoch 315/500\n",
            "7/7 [==============================] - 0s 372us/sample - loss: 4.4564 - acc: 0.7143\n",
            "Epoch 316/500\n",
            "7/7 [==============================] - 0s 363us/sample - loss: 4.4563 - acc: 0.7143\n",
            "Epoch 317/500\n",
            "7/7 [==============================] - 0s 350us/sample - loss: 4.4562 - acc: 0.7143\n",
            "Epoch 318/500\n",
            "7/7 [==============================] - 0s 275us/sample - loss: 4.4560 - acc: 0.7143\n",
            "Epoch 319/500\n",
            "7/7 [==============================] - 0s 354us/sample - loss: 4.4559 - acc: 0.7143\n",
            "Epoch 320/500\n",
            "7/7 [==============================] - 0s 423us/sample - loss: 4.4558 - acc: 0.7143\n",
            "Epoch 321/500\n",
            "7/7 [==============================] - 0s 479us/sample - loss: 4.4557 - acc: 0.7143\n",
            "Epoch 322/500\n",
            "7/7 [==============================] - 0s 441us/sample - loss: 4.4556 - acc: 0.7143\n",
            "Epoch 323/500\n",
            "7/7 [==============================] - 0s 361us/sample - loss: 4.4554 - acc: 0.7143\n",
            "Epoch 324/500\n",
            "7/7 [==============================] - 0s 354us/sample - loss: 4.4553 - acc: 0.7143\n",
            "Epoch 325/500\n",
            "7/7 [==============================] - 0s 447us/sample - loss: 4.4552 - acc: 0.7143\n",
            "Epoch 326/500\n",
            "7/7 [==============================] - 0s 464us/sample - loss: 4.4551 - acc: 0.7143\n",
            "Epoch 327/500\n",
            "7/7 [==============================] - 0s 400us/sample - loss: 4.4550 - acc: 0.7143\n",
            "Epoch 328/500\n",
            "7/7 [==============================] - 0s 496us/sample - loss: 4.4548 - acc: 0.7143\n",
            "Epoch 329/500\n",
            "7/7 [==============================] - 0s 355us/sample - loss: 4.4547 - acc: 0.7143\n",
            "Epoch 330/500\n",
            "7/7 [==============================] - 0s 503us/sample - loss: 4.4546 - acc: 0.7143\n",
            "Epoch 331/500\n",
            "7/7 [==============================] - 0s 343us/sample - loss: 4.4545 - acc: 0.7143\n",
            "Epoch 332/500\n",
            "7/7 [==============================] - 0s 451us/sample - loss: 4.4544 - acc: 0.7143\n",
            "Epoch 333/500\n",
            "7/7 [==============================] - 0s 402us/sample - loss: 4.4542 - acc: 0.7143\n",
            "Epoch 334/500\n",
            "7/7 [==============================] - 0s 316us/sample - loss: 4.4541 - acc: 0.7143\n",
            "Epoch 335/500\n",
            "7/7 [==============================] - 0s 420us/sample - loss: 4.4540 - acc: 0.7143\n",
            "Epoch 336/500\n",
            "7/7 [==============================] - 0s 251us/sample - loss: 4.4539 - acc: 0.7143\n",
            "Epoch 337/500\n",
            "7/7 [==============================] - 0s 393us/sample - loss: 4.4537 - acc: 0.7143\n",
            "Epoch 338/500\n",
            "7/7 [==============================] - 0s 386us/sample - loss: 4.4536 - acc: 0.7143\n",
            "Epoch 339/500\n",
            "7/7 [==============================] - 0s 311us/sample - loss: 4.4535 - acc: 0.7143\n",
            "Epoch 340/500\n",
            "7/7 [==============================] - 0s 241us/sample - loss: 4.4534 - acc: 0.7143\n",
            "Epoch 341/500\n",
            "7/7 [==============================] - 0s 409us/sample - loss: 4.4533 - acc: 0.7143\n",
            "Epoch 342/500\n",
            "7/7 [==============================] - 0s 227us/sample - loss: 4.4531 - acc: 0.7143\n",
            "Epoch 343/500\n",
            "7/7 [==============================] - 0s 428us/sample - loss: 4.4530 - acc: 0.7143\n",
            "Epoch 344/500\n",
            "7/7 [==============================] - 0s 361us/sample - loss: 4.4529 - acc: 0.7143\n",
            "Epoch 345/500\n",
            "7/7 [==============================] - 0s 368us/sample - loss: 4.4528 - acc: 0.7143\n",
            "Epoch 346/500\n",
            "7/7 [==============================] - 0s 394us/sample - loss: 4.4526 - acc: 0.7143\n",
            "Epoch 347/500\n",
            "7/7 [==============================] - 0s 313us/sample - loss: 4.4525 - acc: 0.7143\n",
            "Epoch 348/500\n",
            "7/7 [==============================] - 0s 231us/sample - loss: 4.4524 - acc: 0.7143\n",
            "Epoch 349/500\n",
            "7/7 [==============================] - 0s 226us/sample - loss: 4.4523 - acc: 0.7143\n",
            "Epoch 350/500\n",
            "7/7 [==============================] - 0s 260us/sample - loss: 4.4521 - acc: 0.7143\n",
            "Epoch 351/500\n",
            "7/7 [==============================] - 0s 264us/sample - loss: 4.4520 - acc: 0.7143\n",
            "Epoch 352/500\n",
            "7/7 [==============================] - 0s 294us/sample - loss: 4.4519 - acc: 0.7143\n",
            "Epoch 353/500\n",
            "7/7 [==============================] - 0s 201us/sample - loss: 4.4518 - acc: 0.7143\n",
            "Epoch 354/500\n",
            "7/7 [==============================] - 0s 315us/sample - loss: 4.4516 - acc: 0.7143\n",
            "Epoch 355/500\n",
            "7/7 [==============================] - 0s 277us/sample - loss: 4.4515 - acc: 0.7143\n",
            "Epoch 356/500\n",
            "7/7 [==============================] - 0s 491us/sample - loss: 4.4514 - acc: 0.7143\n",
            "Epoch 357/500\n",
            "7/7 [==============================] - 0s 391us/sample - loss: 4.4513 - acc: 0.7143\n",
            "Epoch 358/500\n",
            "7/7 [==============================] - 0s 304us/sample - loss: 4.4511 - acc: 0.7143\n",
            "Epoch 359/500\n",
            "7/7 [==============================] - 0s 828us/sample - loss: 4.4510 - acc: 0.7143\n",
            "Epoch 360/500\n",
            "7/7 [==============================] - 0s 323us/sample - loss: 4.4509 - acc: 0.7143\n",
            "Epoch 361/500\n",
            "7/7 [==============================] - 0s 310us/sample - loss: 4.4507 - acc: 0.7143\n",
            "Epoch 362/500\n",
            "7/7 [==============================] - 0s 413us/sample - loss: 4.4506 - acc: 0.7143\n",
            "Epoch 363/500\n",
            "7/7 [==============================] - 0s 292us/sample - loss: 4.4505 - acc: 0.7143\n",
            "Epoch 364/500\n",
            "7/7 [==============================] - 0s 339us/sample - loss: 4.4504 - acc: 0.7143\n",
            "Epoch 365/500\n",
            "7/7 [==============================] - 0s 286us/sample - loss: 4.4502 - acc: 0.7143\n",
            "Epoch 366/500\n",
            "7/7 [==============================] - 0s 287us/sample - loss: 4.4501 - acc: 0.7143\n",
            "Epoch 367/500\n",
            "7/7 [==============================] - 0s 284us/sample - loss: 4.4500 - acc: 0.7143\n",
            "Epoch 368/500\n",
            "7/7 [==============================] - 0s 327us/sample - loss: 4.4499 - acc: 0.7143\n",
            "Epoch 369/500\n",
            "7/7 [==============================] - 0s 385us/sample - loss: 4.4497 - acc: 0.7143\n",
            "Epoch 370/500\n",
            "7/7 [==============================] - 0s 278us/sample - loss: 4.4496 - acc: 0.7143\n",
            "Epoch 371/500\n",
            "7/7 [==============================] - 0s 258us/sample - loss: 4.4495 - acc: 0.7143\n",
            "Epoch 372/500\n",
            "7/7 [==============================] - 0s 267us/sample - loss: 4.4493 - acc: 0.7143\n",
            "Epoch 373/500\n",
            "7/7 [==============================] - 0s 243us/sample - loss: 4.4492 - acc: 0.7143\n",
            "Epoch 374/500\n",
            "7/7 [==============================] - 0s 328us/sample - loss: 4.4491 - acc: 0.7143\n",
            "Epoch 375/500\n",
            "7/7 [==============================] - 0s 294us/sample - loss: 4.4490 - acc: 0.7143\n",
            "Epoch 376/500\n",
            "7/7 [==============================] - 0s 309us/sample - loss: 4.4488 - acc: 0.7143\n",
            "Epoch 377/500\n",
            "7/7 [==============================] - 0s 247us/sample - loss: 4.4487 - acc: 0.7143\n",
            "Epoch 378/500\n",
            "7/7 [==============================] - 0s 278us/sample - loss: 4.4486 - acc: 0.7143\n",
            "Epoch 379/500\n",
            "7/7 [==============================] - 0s 358us/sample - loss: 4.4484 - acc: 0.7143\n",
            "Epoch 380/500\n",
            "7/7 [==============================] - 0s 275us/sample - loss: 4.4483 - acc: 0.7143\n",
            "Epoch 381/500\n",
            "7/7 [==============================] - 0s 359us/sample - loss: 4.4482 - acc: 0.7143\n",
            "Epoch 382/500\n",
            "7/7 [==============================] - 0s 273us/sample - loss: 4.4480 - acc: 0.7143\n",
            "Epoch 383/500\n",
            "7/7 [==============================] - 0s 263us/sample - loss: 4.4479 - acc: 0.7143\n",
            "Epoch 384/500\n",
            "7/7 [==============================] - 0s 341us/sample - loss: 4.4478 - acc: 0.7143\n",
            "Epoch 385/500\n",
            "7/7 [==============================] - 0s 257us/sample - loss: 4.4477 - acc: 0.7143\n",
            "Epoch 386/500\n",
            "7/7 [==============================] - 0s 386us/sample - loss: 4.4475 - acc: 0.7143\n",
            "Epoch 387/500\n",
            "7/7 [==============================] - 0s 290us/sample - loss: 4.4474 - acc: 0.7143\n",
            "Epoch 388/500\n",
            "7/7 [==============================] - 0s 225us/sample - loss: 4.4473 - acc: 0.7143\n",
            "Epoch 389/500\n",
            "7/7 [==============================] - 0s 307us/sample - loss: 4.4471 - acc: 0.7143\n",
            "Epoch 390/500\n",
            "7/7 [==============================] - 0s 390us/sample - loss: 4.4470 - acc: 0.7143\n",
            "Epoch 391/500\n",
            "7/7 [==============================] - 0s 270us/sample - loss: 4.4469 - acc: 0.7143\n",
            "Epoch 392/500\n",
            "7/7 [==============================] - 0s 322us/sample - loss: 4.4467 - acc: 0.7143\n",
            "Epoch 393/500\n",
            "7/7 [==============================] - 0s 376us/sample - loss: 4.4466 - acc: 0.7143\n",
            "Epoch 394/500\n",
            "7/7 [==============================] - 0s 240us/sample - loss: 4.4465 - acc: 0.7143\n",
            "Epoch 395/500\n",
            "7/7 [==============================] - 0s 321us/sample - loss: 4.4463 - acc: 0.7143\n",
            "Epoch 396/500\n",
            "7/7 [==============================] - 0s 410us/sample - loss: 4.4462 - acc: 0.7143\n",
            "Epoch 397/500\n",
            "7/7 [==============================] - 0s 238us/sample - loss: 4.4461 - acc: 0.7143\n",
            "Epoch 398/500\n",
            "7/7 [==============================] - 0s 422us/sample - loss: 4.4459 - acc: 0.7143\n",
            "Epoch 399/500\n",
            "7/7 [==============================] - 0s 181us/sample - loss: 4.4458 - acc: 0.7143\n",
            "Epoch 400/500\n",
            "7/7 [==============================] - 0s 407us/sample - loss: 4.4457 - acc: 0.7143\n",
            "Epoch 401/500\n",
            "7/7 [==============================] - 0s 256us/sample - loss: 4.4455 - acc: 0.7143\n",
            "Epoch 402/500\n",
            "7/7 [==============================] - 0s 333us/sample - loss: 4.4454 - acc: 0.7143\n",
            "Epoch 403/500\n",
            "7/7 [==============================] - 0s 417us/sample - loss: 4.4453 - acc: 0.7143\n",
            "Epoch 404/500\n",
            "7/7 [==============================] - 0s 414us/sample - loss: 4.4451 - acc: 0.7143\n",
            "Epoch 405/500\n",
            "7/7 [==============================] - 0s 269us/sample - loss: 4.4450 - acc: 0.7143\n",
            "Epoch 406/500\n",
            "7/7 [==============================] - 0s 220us/sample - loss: 4.4449 - acc: 0.7143\n",
            "Epoch 407/500\n",
            "7/7 [==============================] - 0s 373us/sample - loss: 4.4447 - acc: 0.7143\n",
            "Epoch 408/500\n",
            "7/7 [==============================] - 0s 210us/sample - loss: 4.4446 - acc: 0.7143\n",
            "Epoch 409/500\n",
            "7/7 [==============================] - 0s 397us/sample - loss: 4.4445 - acc: 0.7143\n",
            "Epoch 410/500\n",
            "7/7 [==============================] - 0s 226us/sample - loss: 4.4443 - acc: 0.7143\n",
            "Epoch 411/500\n",
            "7/7 [==============================] - 0s 243us/sample - loss: 4.4442 - acc: 0.7143\n",
            "Epoch 412/500\n",
            "7/7 [==============================] - 0s 708us/sample - loss: 4.4440 - acc: 0.7143\n",
            "Epoch 413/500\n",
            "7/7 [==============================] - 0s 565us/sample - loss: 4.4439 - acc: 0.7143\n",
            "Epoch 414/500\n",
            "7/7 [==============================] - 0s 573us/sample - loss: 4.4438 - acc: 0.7143\n",
            "Epoch 415/500\n",
            "7/7 [==============================] - 0s 433us/sample - loss: 4.4436 - acc: 0.7143\n",
            "Epoch 416/500\n",
            "7/7 [==============================] - 0s 508us/sample - loss: 4.4435 - acc: 0.7143\n",
            "Epoch 417/500\n",
            "7/7 [==============================] - 0s 293us/sample - loss: 4.4434 - acc: 0.7143\n",
            "Epoch 418/500\n",
            "7/7 [==============================] - 0s 512us/sample - loss: 4.4432 - acc: 0.7143\n",
            "Epoch 419/500\n",
            "7/7 [==============================] - 0s 468us/sample - loss: 4.4431 - acc: 0.7143\n",
            "Epoch 420/500\n",
            "7/7 [==============================] - 0s 305us/sample - loss: 4.4430 - acc: 0.7143\n",
            "Epoch 421/500\n",
            "7/7 [==============================] - 0s 245us/sample - loss: 4.4428 - acc: 0.7143\n",
            "Epoch 422/500\n",
            "7/7 [==============================] - 0s 435us/sample - loss: 4.4427 - acc: 0.7143\n",
            "Epoch 423/500\n",
            "7/7 [==============================] - 0s 450us/sample - loss: 4.4425 - acc: 0.7143\n",
            "Epoch 424/500\n",
            "7/7 [==============================] - 0s 428us/sample - loss: 4.4424 - acc: 0.7143\n",
            "Epoch 425/500\n",
            "7/7 [==============================] - 0s 322us/sample - loss: 4.4423 - acc: 0.7143\n",
            "Epoch 426/500\n",
            "7/7 [==============================] - 0s 244us/sample - loss: 4.4421 - acc: 0.7143\n",
            "Epoch 427/500\n",
            "7/7 [==============================] - 0s 470us/sample - loss: 4.4420 - acc: 0.7143\n",
            "Epoch 428/500\n",
            "7/7 [==============================] - 0s 369us/sample - loss: 4.4419 - acc: 0.7143\n",
            "Epoch 429/500\n",
            "7/7 [==============================] - 0s 423us/sample - loss: 4.4417 - acc: 0.7143\n",
            "Epoch 430/500\n",
            "7/7 [==============================] - 0s 323us/sample - loss: 4.4416 - acc: 0.7143\n",
            "Epoch 431/500\n",
            "7/7 [==============================] - 0s 202us/sample - loss: 4.4414 - acc: 0.7143\n",
            "Epoch 432/500\n",
            "7/7 [==============================] - 0s 496us/sample - loss: 4.4413 - acc: 0.7143\n",
            "Epoch 433/500\n",
            "7/7 [==============================] - 0s 375us/sample - loss: 4.4412 - acc: 0.7143\n",
            "Epoch 434/500\n",
            "7/7 [==============================] - 0s 291us/sample - loss: 4.4410 - acc: 0.7143\n",
            "Epoch 435/500\n",
            "7/7 [==============================] - 0s 364us/sample - loss: 4.4409 - acc: 0.7143\n",
            "Epoch 436/500\n",
            "7/7 [==============================] - 0s 208us/sample - loss: 4.4407 - acc: 0.7143\n",
            "Epoch 437/500\n",
            "7/7 [==============================] - 0s 288us/sample - loss: 4.4406 - acc: 0.7143\n",
            "Epoch 438/500\n",
            "7/7 [==============================] - 0s 288us/sample - loss: 4.4405 - acc: 0.7143\n",
            "Epoch 439/500\n",
            "7/7 [==============================] - 0s 304us/sample - loss: 4.4403 - acc: 0.7143\n",
            "Epoch 440/500\n",
            "7/7 [==============================] - 0s 299us/sample - loss: 4.4402 - acc: 0.7143\n",
            "Epoch 441/500\n",
            "7/7 [==============================] - 0s 245us/sample - loss: 4.4400 - acc: 0.7143\n",
            "Epoch 442/500\n",
            "7/7 [==============================] - 0s 202us/sample - loss: 4.4399 - acc: 0.7143\n",
            "Epoch 443/500\n",
            "7/7 [==============================] - 0s 357us/sample - loss: 4.4398 - acc: 0.7143\n",
            "Epoch 444/500\n",
            "7/7 [==============================] - 0s 236us/sample - loss: 4.4396 - acc: 0.7143\n",
            "Epoch 445/500\n",
            "7/7 [==============================] - 0s 368us/sample - loss: 4.4395 - acc: 0.7143\n",
            "Epoch 446/500\n",
            "7/7 [==============================] - 0s 519us/sample - loss: 4.4393 - acc: 0.7143\n",
            "Epoch 447/500\n",
            "7/7 [==============================] - 0s 406us/sample - loss: 4.4392 - acc: 0.7143\n",
            "Epoch 448/500\n",
            "7/7 [==============================] - 0s 365us/sample - loss: 4.4391 - acc: 0.7143\n",
            "Epoch 449/500\n",
            "7/7 [==============================] - 0s 412us/sample - loss: 4.4389 - acc: 0.7143\n",
            "Epoch 450/500\n",
            "7/7 [==============================] - 0s 424us/sample - loss: 4.4388 - acc: 0.7143\n",
            "Epoch 451/500\n",
            "7/7 [==============================] - 0s 291us/sample - loss: 4.4386 - acc: 0.7143\n",
            "Epoch 452/500\n",
            "7/7 [==============================] - 0s 338us/sample - loss: 4.4385 - acc: 0.7143\n",
            "Epoch 453/500\n",
            "7/7 [==============================] - 0s 381us/sample - loss: 4.4383 - acc: 0.7143\n",
            "Epoch 454/500\n",
            "7/7 [==============================] - 0s 384us/sample - loss: 4.4382 - acc: 0.7143\n",
            "Epoch 455/500\n",
            "7/7 [==============================] - 0s 408us/sample - loss: 4.4381 - acc: 0.7143\n",
            "Epoch 456/500\n",
            "7/7 [==============================] - 0s 541us/sample - loss: 4.4379 - acc: 0.7143\n",
            "Epoch 457/500\n",
            "7/7 [==============================] - 0s 241us/sample - loss: 4.4378 - acc: 0.7143\n",
            "Epoch 458/500\n",
            "7/7 [==============================] - 0s 227us/sample - loss: 4.4376 - acc: 0.7143\n",
            "Epoch 459/500\n",
            "7/7 [==============================] - 0s 254us/sample - loss: 4.4376 - acc: 0.7143\n",
            "Epoch 460/500\n",
            "7/7 [==============================] - 0s 259us/sample - loss: 4.4374 - acc: 0.7143\n",
            "Epoch 461/500\n",
            "7/7 [==============================] - 0s 275us/sample - loss: 4.4373 - acc: 0.7143\n",
            "Epoch 462/500\n",
            "7/7 [==============================] - 0s 209us/sample - loss: 4.4372 - acc: 0.7143\n",
            "Epoch 463/500\n",
            "7/7 [==============================] - 0s 281us/sample - loss: 4.4370 - acc: 0.7143\n",
            "Epoch 464/500\n",
            "7/7 [==============================] - 0s 230us/sample - loss: 4.4369 - acc: 0.7143\n",
            "Epoch 465/500\n",
            "7/7 [==============================] - 0s 238us/sample - loss: 4.4368 - acc: 0.7143\n",
            "Epoch 466/500\n",
            "7/7 [==============================] - 0s 277us/sample - loss: 4.4367 - acc: 0.7143\n",
            "Epoch 467/500\n",
            "7/7 [==============================] - 0s 399us/sample - loss: 4.4366 - acc: 0.7143\n",
            "Epoch 468/500\n",
            "7/7 [==============================] - 0s 501us/sample - loss: 4.4364 - acc: 0.7143\n",
            "Epoch 469/500\n",
            "7/7 [==============================] - 0s 401us/sample - loss: 4.4363 - acc: 0.7143\n",
            "Epoch 470/500\n",
            "7/7 [==============================] - 0s 408us/sample - loss: 4.4362 - acc: 0.7143\n",
            "Epoch 471/500\n",
            "7/7 [==============================] - 0s 411us/sample - loss: 4.4361 - acc: 0.7143\n",
            "Epoch 472/500\n",
            "7/7 [==============================] - 0s 473us/sample - loss: 4.4359 - acc: 0.7143\n",
            "Epoch 473/500\n",
            "7/7 [==============================] - 0s 526us/sample - loss: 4.4358 - acc: 0.7143\n",
            "Epoch 474/500\n",
            "7/7 [==============================] - 0s 474us/sample - loss: 4.4357 - acc: 0.7143\n",
            "Epoch 475/500\n",
            "7/7 [==============================] - 0s 301us/sample - loss: 4.4356 - acc: 0.7143\n",
            "Epoch 476/500\n",
            "7/7 [==============================] - 0s 200us/sample - loss: 4.4354 - acc: 0.7143\n",
            "Epoch 477/500\n",
            "7/7 [==============================] - 0s 256us/sample - loss: 4.4353 - acc: 0.7143\n",
            "Epoch 478/500\n",
            "7/7 [==============================] - 0s 232us/sample - loss: 4.4352 - acc: 0.7143\n",
            "Epoch 479/500\n",
            "7/7 [==============================] - 0s 286us/sample - loss: 4.4351 - acc: 0.7143\n",
            "Epoch 480/500\n",
            "7/7 [==============================] - 0s 320us/sample - loss: 4.4349 - acc: 0.7143\n",
            "Epoch 481/500\n",
            "7/7 [==============================] - 0s 302us/sample - loss: 4.4348 - acc: 0.7143\n",
            "Epoch 482/500\n",
            "7/7 [==============================] - 0s 425us/sample - loss: 4.4347 - acc: 0.7143\n",
            "Epoch 483/500\n",
            "7/7 [==============================] - 0s 356us/sample - loss: 4.4346 - acc: 0.7143\n",
            "Epoch 484/500\n",
            "7/7 [==============================] - 0s 439us/sample - loss: 4.4344 - acc: 0.7143\n",
            "Epoch 485/500\n",
            "7/7 [==============================] - 0s 377us/sample - loss: 4.4343 - acc: 0.7143\n",
            "Epoch 486/500\n",
            "7/7 [==============================] - 0s 365us/sample - loss: 4.4342 - acc: 0.7143\n",
            "Epoch 487/500\n",
            "7/7 [==============================] - 0s 471us/sample - loss: 4.4341 - acc: 0.7143\n",
            "Epoch 488/500\n",
            "7/7 [==============================] - 0s 364us/sample - loss: 4.4340 - acc: 0.7143\n",
            "Epoch 489/500\n",
            "7/7 [==============================] - 0s 480us/sample - loss: 4.4338 - acc: 0.7143\n",
            "Epoch 490/500\n",
            "7/7 [==============================] - 0s 228us/sample - loss: 4.4337 - acc: 0.7143\n",
            "Epoch 491/500\n",
            "7/7 [==============================] - 0s 281us/sample - loss: 4.4336 - acc: 0.7143\n",
            "Epoch 492/500\n",
            "7/7 [==============================] - 0s 465us/sample - loss: 4.4334 - acc: 0.7143\n",
            "Epoch 493/500\n",
            "7/7 [==============================] - 0s 354us/sample - loss: 4.4333 - acc: 0.7143\n",
            "Epoch 494/500\n",
            "7/7 [==============================] - 0s 328us/sample - loss: 4.4332 - acc: 0.7143\n",
            "Epoch 495/500\n",
            "7/7 [==============================] - 0s 375us/sample - loss: 4.4331 - acc: 0.7143\n",
            "Epoch 496/500\n",
            "7/7 [==============================] - 0s 474us/sample - loss: 4.4330 - acc: 0.7143\n",
            "Epoch 497/500\n",
            "7/7 [==============================] - 0s 911us/sample - loss: 4.4328 - acc: 0.7143\n",
            "Epoch 498/500\n",
            "7/7 [==============================] - 0s 380us/sample - loss: 4.4327 - acc: 0.7143\n",
            "Epoch 499/500\n",
            "7/7 [==============================] - 0s 359us/sample - loss: 4.4326 - acc: 0.7143\n",
            "Epoch 500/500\n",
            "7/7 [==============================] - 0s 401us/sample - loss: 4.4325 - acc: 0.7143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAWU9mwfFnh2",
        "colab_type": "text"
      },
      "source": [
        "### Build a Tensor Keras Perceptron\n",
        "\n",
        "Try to match the architecture we used on Monday - inputs nodes and one output node. Apply this architecture to the XOR-ish dataset above. \n",
        "\n",
        "After fitting your model answer these questions: \n",
        "\n",
        "Are you able to achieve the same results as a bigger architecture from the first part of the assignment? Why is this disparity the case? What properties of the XOR dataset would cause this disparity? \n",
        "\n",
        "Now extrapolate this behavior on a much larger dataset in terms of features. What kind of architecture decisions could we make to avoid the problems the XOR dataset presents at scale? \n",
        "\n",
        "*Note:* The bias term is baked in by default in the Dense layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL_A5TLfFnh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Compare "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8b-r70o8p2Dm"
      },
      "source": [
        "## Try building/training a more complex MLP on a bigger dataset.\n",
        "\n",
        "Use TensorFlow Keras & the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to build the canonical handwriting digit recognizer and see what kind of accuracy you can achieve. \n",
        "\n",
        "If you need inspiration, the Internet is chalk-full of tutorials, but I want you to see how far you can get on your own first. I've linked to the original MNIST dataset above but it will probably be easier to download data through a neural network library. If you reference outside resources make sure you understand every line of code that you're using from other sources, and share with your fellow students helpful resources that you find.\n",
        "\n",
        "\n",
        "### Parts\n",
        "1. Gathering & Transforming the Data\n",
        "2. Making MNIST a Binary Problem\n",
        "3. Estimating your Neural Network (the part you focus on)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYIk4y-zFnh_",
        "colab_type": "text"
      },
      "source": [
        "### Gathering the Data \n",
        "\n",
        "`keras` has a handy method to pull the mnist dataset for you. You'll notice that each observation is a 28x28 arrary which represents an image. Although most Neural Network frameworks can handle higher dimensional data, that is more overhead than necessary for us. We need to flatten the image to one long row which will be 784 values (28X28). Basically, you will be appending each row to one another to make on really long row. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KooK4AM9FniB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlXHF4t4FniJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDEfCeLaFniP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "60857438-8f1d-456a-886b-ccd7305ae800"
      },
      "source": [
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXFltn7wFniV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0], img_rows * img_cols)\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows * img_cols)\n",
        "\n",
        "# Normalize Our Data\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTd_qAV9FniZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4e8922c-dcb0-45de-82e4-e7baeee1feae"
      },
      "source": [
        "# Now the data should be in a format you're more familiar with\n",
        "x_train.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyikLsO6JTjG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "42bcfaaa-ad7d-42dc-e5c8-a8865ba89eb4"
      },
      "source": [
        "x_train"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGrRG-bJFnif",
        "colab_type": "text"
      },
      "source": [
        "### Making MNIST a Binary Problem \n",
        "MNIST is multiclass classification problem; however we haven't covered all the necessary techniques to handle this yet. You would need to one-hot encode the target, use a different loss metric, and use softmax activations for the last layer. This is all stuff we'll cover later this week, but let us simplify the problem for now: Zero or all else."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpCgrBXJFnii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "y_temp = np.zeros(y_train.shape)\n",
        "y_temp[np.where(y_train == 0.0)[0]] = 1\n",
        "y_train = y_temp\n",
        "\n",
        "y_temp = np.zeros(y_test.shape)\n",
        "y_temp[np.where(y_test == 0.0)[0]] = 1\n",
        "y_test = y_temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxliESPGFnio",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9729bb42-17e0-4834-ee33-61ce10f887be"
      },
      "source": [
        "# A Nice Binary target for ya to work with\n",
        "y_train"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 0., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIEIV6cOFniv",
        "colab_type": "text"
      },
      "source": [
        "### Estimating Your `net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5MOPtYdk1HgA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "af0dd656-578f-4990-f145-53b458ae9010"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(100, activation='relu', input_dim=784),\n",
        "    Dense(1, activation='relu')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "results = model.fit(x_train,y_train, epochs=100, validation_data=(x_test,y_test))\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 5s 89us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 5s 84us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 5s 83us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 5s 84us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 1.5227 - acc: 0.9013 - val_loss: 1.5116 - val_acc: 0.9020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t5Za1GsUoWu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "984f2202-df3b-4a5c-d7a8-f0eaeba7cb48"
      },
      "source": [
        "model.evaluate(x_test,y_test)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.0341 - acc: 0.9975\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.03412450841665268, 0.9975]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S29CZ5KZMzo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "1dd0a141-3d01-4223-944e-6dff8d90461c"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       ...,\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUcp4RLTZV_P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a66be16-0c54-4c68-f438-fece2acb5573"
      },
      "source": [
        "y_test"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FwlRJSfBlCvy"
      },
      "source": [
        "## Stretch Goals: \n",
        "\n",
        "- Make MNIST a multiclass problem using cross entropy & soft-max\n",
        "- Implement Cross Validation model evaluation on your MNIST implementation \n",
        "- Research different [Gradient Descent Based Optimizers](https://keras.io/optimizers/)\n",
        " - [Siraj Raval the evolution of gradient descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
        "- Build a housing price estimation model using a neural network. How does its accuracy compare with the regression models that we fit earlier on in class?"
      ]
    }
  ]
}